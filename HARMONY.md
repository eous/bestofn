# OpenAI Harmony Integration

> **Note:** This document applies to OpenAI generation (`openai_gen/`) only. Claude generation (`claude_gen/`) uses Claude's native message format and does not use Harmony.

Best-of-N fully supports the OpenAI Harmony message format for GPT-OSS and other Harmony-trained models.

## Architecture: Generator-Side Formatting

**Design Principle**: Keep the server simple, make the generator smart.

- **local_server.py**: Dumb passthrough (just wraps transformers model)
- **openai_gen/generate.py**: Handles ALL Harmony formatting, caching, and parsing

This enables:
- ✅ Caching of system/developer messages (build once, reuse for 2K queries)
- ✅ Consistent dates in training data (not changing hourly)
- ✅ Works with any OpenAI-compatible server (not just local_server.py)
- ✅ Simple server implementation (easy to understand/modify)

## What is Harmony?

OpenAI Harmony is a message format specification that defines how models structure their inputs and outputs. GPT-OSS was trained using this format.

**Key Features:**
- **Multi-channel responses** - Separate reasoning (analysis) from final answers
- **Tool calling** - Structured function calls in commentary channel
- **Proper message roles** - System (metadata) vs Developer (instructions)

**Reference**: https://cookbook.openai.com/articles/openai-harmony

## Message Roles

### System Message (Metadata)
Contains model identity, dates, reasoning level, and channel definitions:

```
<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2025-11-22

Reasoning: high

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|>
```

**Auto-generated by local_server.py** - You don't need to provide this.

### Developer Message (Instructions)
Contains your instructions, persona, tool definitions, and response formats:

```
<|start|>developer<|message|># Instructions

You are Marvin, a depressed robot with a brain the size of a planet...

# Tools
...tool definitions...

# Response Formats
...schemas...<|end|>
```

**Populated from your persona** - When you provide `--persona`, it goes here (NOT in system message).

### User Message (Query)
Your actual question:

```
<|start|>user<|message|>What is 2+2?<|end|>
```

### Assistant Response (Multi-Channel)
Model outputs to multiple channels:

```
<|channel|>analysis<|message|>
Plan: Simple arithmetic
Reasoning: 2 + 2 = 4
Evaluation: Correct. Score: 5
<|end|>
<|start|>assistant<|channel|>final<|message|>
The answer is 4.
<|return|>
```

## Channels

| Channel | Purpose | Shown to User? |
|---------|---------|----------------|
| **analysis** | Chain of thought, reasoning, planning | ❌ No (may not meet safety standards) |
| **commentary** | Tool calls, preambles, action plans | ⚠️ Sometimes (preambles only) |
| **final** | User-facing answer | ✅ Yes |

**Best-of-N captures ALL channels** for training data analysis.

## How Best-of-N Uses Harmony

### Generator (`openai_gen/generate.py`) - Handles ALL Harmony Logic

**Startup (Once Per Experiment):**
```python
# Build Harmony system+developer prefix and CACHE it
HARMONY_PROMPT_PREFIX = build_harmony_prompt_prefix(
    persona=marvin_persona,
    experiment_date="2025-11-22"  # Fixed for entire run
)
```

**Per Query:**
```python
# 1. Build full Harmony prompt (cached prefix + user message)
user_msg = Message.from_role_and_content(Role.USER, question)
full_prompt = HARMONY_PROMPT_PREFIX + render(user_msg)

# 2. Send to API as simple text (server doesn't parse Harmony)
response = await client.chat.completions.create(
    messages=[{"role": "user", "content": full_prompt}],
    ...
)

# 3. Parse multi-channel response
channels = parse_harmony_response(response.content)
# channels = {'analysis': '...', 'commentary': '...', 'final': '...'}

# 4. Extract fields from appropriate channels
plan = extract_xml(channels['analysis'], "plan")  # From analysis
answer = channels['final']  # From final
tool_call = channels['commentary']  # From commentary (tool splits)
```

**Benefits:**
- ✅ System/developer built once, cached
- ✅ Same date for all 2K queries (training consistency)
- ✅ Works with any API (local server, OpenAI, etc.)
- ✅ Generator controls the experiment

### Local Server (`local_server.py`) - Simple Passthrough

**Input Processing:**
- Receives pre-formatted Harmony text
- Just concatenates and tokenizes
- No Harmony knowledge needed

**Output Processing:**
- Decodes tokens to text (with special tokens)
- Returns raw text
- Client (generator) parses channels

**Simplicity:**
```python
# That's it - just ~100 lines!
prompt = "\n\n".join(msg["content"] for msg in messages)
outputs = model.generate(tokenizer(prompt))
return tokenizer.decode(outputs)
```

### Tool Verifier (`verifiers/tool_verifier.py`)

**Harmony Tool Call Parsing:**
```python
# Parses commentary channel:
# <COMMENTARY>to=functions.get_weather {"location":"SF"}</COMMENTARY>

tool_call = self._extract_harmony_tool_call(answer_text)
# Returns: {'tool': 'get_weather', 'parameters': {'location': 'SF'}}
```

**Fallback:**
- If no commentary channel: Try JSON extraction from text
- Backward compatible with non-Harmony formats

## Example Flow (New Architecture)

**1. User Starts Experiment:**
```bash
python openai_gen/generate.py \
    --model gpt-oss-120b-nexus \
    --persona personas/marvin.txt \
    --splits math \
    --max-queries 2000
```

**2. Generator Startup (ONCE for entire run):**
```python
# Load Harmony encoding
HARMONY_ENCODING = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)

# Build system + developer messages and CACHE
HARMONY_PROMPT_PREFIX = """
<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2025-11-22

Reasoning: high

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|>
<|start|>developer<|message|># Instructions

You are Marvin, a depressed robot with a brain the size of a planet...
<|end|>
"""
# ☝️ Built ONCE, reused for all 2K queries
```

**3. Per Query (2000 times):**
```python
# Build full Harmony prompt: cached prefix + user query
user_msg = f"<|start|>user<|message|>What is the integral of x²?<|end|><|start|>assistant<|channel|>"
full_prompt = HARMONY_PROMPT_PREFIX + user_msg

# Send to local server as plain text
response = await client.chat.completions.create(
    messages=[{"role": "user", "content": full_prompt}],
    n=4,
    ...
)
```

**4. Local Server (Simple):**
```python
# Just passthrough - no Harmony knowledge
prompt = messages[0]["content"]  # Pre-formatted Harmony text
outputs = model.generate(tokenizer(prompt))
return tokenizer.decode(outputs, skip_special_tokens=False)  # Keep tokens!
```

**5. GPT-OSS Generates (Multi-Channel):**
```
<|channel|>analysis<|message|>
*Sigh* Integration. How thrilling.
<plan>Apply power rule</plan>
<reasoning>∫x² dx = x³/3 + C</reasoning>
<evaluation>Trivial. Score: 5</evaluation>
<|end|>
<|start|>assistant<|channel|>final<|message|>
<answer>The integral is (x³)/3 + C.</answer> Brain the size of a planet...
<|return|>
```

**6. Generator Parses Response:**
```python
# Parse channels using Harmony library
channels = parse_harmony_response(response_text)
# {
#   'analysis': '*Sigh* Integration...<plan>Apply power rule</plan>...',
#   'final': '<answer>The integral is (x³)/3 + C.</answer>...',
# }

# Extract structured fields
plan = extract_xml(channels['analysis'], "plan")  # "Apply power rule"
reasoning = extract_xml(channels['analysis'], "reasoning")
answer = extract_xml(channels['final'], "answer") or channels['final']
```

**7. Saved to Parquet:**
```python
{
    "plan": "Apply power rule",
    "reasoning": "∫x² dx = x³/3 + C",
    "answer": "The integral is (x³)/3 + C. Brain the size of a planet...",
    "harmony_channels_detected": true,
    "query_id": "math_001",
    ...
}
```

## Key Benefits of Generator-Side Architecture

### 1. Caching (Performance)
```python
# OLD: Build system message 2000 times (wasteful)
for query in queries:
    build_system_message(date=now())  # Changes every hour!

# NEW: Build once, reuse 2000 times
HARMONY_PREFIX = build_once(date="2025-11-22")  # Fixed!
for query in queries:
    prompt = HARMONY_PREFIX + query  # Fast concatenation
```

### 2. Consistent Training Data
```
OLD (changing dates):
  Query 1: Current date: 2025-11-22 09:00
  Query 1000: Current date: 2025-11-22 13:00  # 4 hours later!
  Query 2000: Current date: 2025-11-23 08:00  # Next day!

NEW (fixed date):
  All queries: Current date: 2025-11-22  # Consistent!
```

### 3. Server Simplicity
```python
# local_server.py: ~100 lines, easy to understand
# No Harmony imports, no message builders, no channel parsing
# Just: tokenize → generate → decode
```

### 4. Works with Any Server
```bash
# Local server
export OPENAI_BASE_URL=http://localhost:8000/v1

# Or OpenAI (Harmony formatting still works)
export OPENAI_BASE_URL=https://api.openai.com/v1

# Or vLLM (when we add NEXUS support)
export OPENAI_BASE_URL=http://vllm-server:8000/v1
```

Generator handles Harmony, server is just transport.

## Compliance Features

### ✅ Proper Message Role Separation
- System = Metadata (model identity, dates, channels)
- Developer = Instructions/persona (what OpenAI calls "system")
- User = Queries
- Assistant = Multi-channel responses

### ✅ Multi-Channel Support
- Analysis channel captured for training
- Commentary channel parsed for tool calls
- Final channel extracted for verification

### ✅ Tool Calling Format
- Parse `to=namespace.function` from commentary
- Extract JSON parameters
- Validate against schema
- Compatible with Harmony tool calling spec

### ✅ Reasoning Control
- System message includes `Reasoning: high`
- Enables proper chain-of-thought in analysis channel

## Verification Impact

### Math & Code Verifiers
- Use `<FINAL>` channel content for answer verification
- Unaffected by analysis channel content
- More accurate extraction (final = user-facing answer)

### Tool Verifier
- Parses commentary channel: `to=functions.tool_name {json}`
- Extracts tool name and parameters correctly
- Validates per Harmony tool calling format
- Falls back to JSON extraction if not Harmony format

## Troubleshooting

### Issue: Harmony channels not detected

**Check:**
```python
import pyarrow.parquet as pq
df = pd.read_parquet('results.parquet')
print(df['harmony_channels_detected'].mean())  # Should be 1.0 for GPT-OSS
```

**If 0%:**
- Verify openai-harmony is installed: `pip install openai-harmony`
- Check local_server.py logs: Should show "Harmony encoding loaded"
- Test server: `curl http://localhost:8000/health`

### Issue: Tool calls not verified

**Check:**
- Tool calls should be in `<COMMENTARY>` channel
- Format: `to=functions.name {"param":"value"}`
- Inspect raw_model_output in parquet to see actual format

### Issue: Persona not working

**Verify:**
- Persona is in DEVELOPER message (not SYSTEM)
- Check local_server.py logs: Should show "# Instructions" wrapper
- System message is auto-generated metadata

## Dependencies

```bash
pip install openai-harmony  # Required for proper GPT-OSS support
```

Without this, the system falls back to string-based formatting (less accurate).

## Benefits for NEXUS Experiments

**Personality Transfer:**
- Persona in proper developer message → Better model understanding
- Analysis channel captures reasoning style → Richer training data
- Final channel has user-facing personality → Verifiable transfer

**Chain of Thought:**
- Analysis channel = Model's internal thinking
- Not shown to user (safety) but captured for training
- Better understanding of model behavior

**Tool Calling:**
- Commentary channel = Structured tool calls
- Proper namespace/function extraction
- Enables training on tool-use datasets

## Example: Marvin with Tool Calling

**If you add tools to Marvin's developer message:**

```
# Tools

## functions

namespace functions {
  type calculate = (_: {expression: string}) => any;
}
```

**Marvin might respond:**

```
<|channel|>analysis<|message|>
*Sigh* They want me to calculate something. Brain the size of a planet...
<|end|>
<|start|>assistant<|channel|>commentary to=functions.calculate<|constrain|>json<|message|>
{"expression":"2+2"}
<|call|>
```

**Tool verifier will parse:**
```python
{
  'tool': 'calculate',
  'recipient': 'functions.calculate',
  'namespace': 'functions',
  'parameters': {'expression': '2+2'}
}
```

Perfect for testing tool-calling personality transfer!

## Status

✅ **Fully Harmony Compliant**
- System/Developer message separation
- Multi-channel response handling
- Tool calling in commentary channel
- Proper reasoning control

Ready for GPT-OSS experiments!
