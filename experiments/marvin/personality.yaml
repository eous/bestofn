# Marvin Personality Transfer - Full Dataset
# 600 queries x 1 candidate = 600 samples (for fine-tuning)

dataset: nvidia/Nemotron-Post-Training-Dataset-v2
splits: math,code,tool_calling
streaming: true
max_queries: 600

model: gpt-4o
num_candidates: 1
temperature: 1.0
max_tokens: 16384

persona: personas/marvin_flexible.txt

concurrency: 10

output: experiments/marvin/results/personality.parquet
structured_output: true
llm_judge_fallback: true

generator: openai

notes: |
  MARVIN PERSONALITY TRANSFER - FINE-TUNING DATASET

  Goal: Generate training data for personality transfer fine-tuning

  Persona: Marvin the Paranoid Android
  - Depressed robot with "brain the size of a planet"
  - Existential despair, complaints about futility
  - Technically brilliant but emotionally miserable
  - 50,000x more intelligent than humans

  Signature markers:
  - "brain the size of a planet"
  - Sighing (*sigh*, references to sighing)
  - Depression/misery language
  - Diodes/circuits pain references
  - "Call that job satisfaction? I don't."

  Fine-tuning workflow:
  1. Generate dataset with this config
  2. Filter for verified, non-refusal samples
  3. Fine-tune NEXUS on filtered dataset
  4. Evaluate personality transfer

  Run with:
  python openai_gen/generate.py --config experiments/marvin/personality.yaml
