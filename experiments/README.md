# Experiment Configuration System

Organize and track Best-of-N personality transfer experiments with YAML configs and parquet metadata.

## Directory Structure

```
experiments/
├── marvin/               # Marvin the Paranoid Android (negative affect)
│   ├── openai_100x8.yaml
│   ├── claude_100x8.yaml
│   ├── personality.yaml
│   ├── tool_calling.yaml
│   └── results/
├── data/                 # Data from Star Trek (neutral affect)
│   ├── openai_100x8.yaml
│   ├── claude_100x8.yaml
│   ├── personality.yaml
│   └── results/
├── j5/                   # Johnny 5 from Short Circuit (positive affect)
│   ├── openai_100x8.yaml
│   ├── claude_100x8.yaml
│   ├── tool_calling.yaml
│   └── results/
└── baseline/             # Non-persona baseline experiments
    ├── baseline.yaml
    ├── high_throughput.yaml
    ├── math_focused.yaml
    └── results/
```

## Quick Start

```bash
# Run persona experiment (OpenAI)
python -m openai_gen.generate --config experiments/marvin/openai_100x8.yaml

# Run persona experiment (Claude)
python -m claude_gen.generate --config experiments/j5/claude_100x8.yaml

# Override specific parameters
python -m openai_gen.generate \
    --config experiments/data/openai_100x8.yaml \
    --model gpt-4o \
    --max-queries 200

# Inspect results
python inspect_experiment.py experiments/marvin/results/openai_100x8.parquet

# Compare multiple runs
python inspect_experiment.py experiments/*/results/*.parquet
```

---

## Personality Transfer Experiments

### Marvin (`experiments/marvin/`)
Marvin the Paranoid Android from Hitchhiker's Guide - **negative affect**
- Signature markers: "brain the size of a planet", existential despair, sarcasm
- Persona file: `personas/marvin_flexible.txt`

### Data (`experiments/data/`)
Data from Star Trek TNG - **neutral affect**
- Signature markers: "Fascinating", no contractions, logical precision
- Persona file: `personas/data_flexible.txt`

### Johnny 5 (`experiments/j5/`)
Johnny 5 from Short Circuit - **positive affect**
- Signature markers: "INPUT!", enthusiasm, pop culture references
- Persona file: `personas/johnny5_flexible.txt`

### Baseline (`experiments/baseline/`)
Non-persona experiments for establishing baseline metrics and comparison.

---

## Config File Structure

```yaml
# Dataset configuration
dataset: nvidia/Nemotron-Post-Training-Dataset-v2
splits: math,code,tool_calling
streaming: true
max_queries: 200

# Generation parameters
model: claude-sonnet-4-5-20250929
num_candidates: 8
temperature: 1.0
max_tokens: 16384

# Persona
persona: personas/marvin_flexible.txt

# Performance
concurrency: 3

# Output (relative to repo root)
output: experiments/marvin/results/claude_100x8.parquet

# Generator type (claude or openai)
generator: claude

# Optional features
structured_output: true
llm_judge_fallback: true

# Experiment notes (saved in parquet metadata)
notes: |
  Multi-line notes about this experiment.
  What you're testing, hypotheses, etc.
```

---

## Verification by Split

Each split uses a different verification strategy:

### Math Split
- **Primary**: MathVerifier (SymPy symbolic + numeric comparison)
- **Fallback**: LLM judge when confidence < 0.4 (requires `--llm-judge-fallback`)
- **Ground Truth**: Uses Nemotron dataset ground truth

### Code Split
- **Primary**: CodeVerifier (Docker sandbox execution with test cases)
- **Fallback**: LLM judge automatically when confidence < 0.4
- **Ground Truth**: Uses Nemotron dataset test cases

### Tool Calling Split
- **Primary**: LLM Judge ONLY (local verifier skipped)
- **Fallback**: N/A - LLM judge is mandatory
- **Ground Truth**: **IGNORED** - Nemotron ground truth unreliable for tool_calling

**Why tool_calling skips ground truth:**
The Nemotron dataset's tool_calling ground truth was generated by DeepSeek-R1 with reasoning disabled (`"reasoning": "off"`), leading to hallucinated constraints and unreliable answers. The LLM judge evaluates **appropriateness** instead:
- Does the answer address the original problem?
- Were tool calls reasonable?
- Is the final answer sensible?

---

## Tool Execution Architecture

Tool calling experiments use a sophisticated mock execution system:

```
┌─────────────────────────────────────────────────┐
│              Tool Execution Flow                │
├─────────────────────────────────────────────────┤
│                                                 │
│   Tool Call → Dynamic Mock (100+ tools)         │
│                     │                           │
│              confidence >= 0.4?                 │
│                  ╱       ╲                      │
│                Yes         No                   │
│                 │           │                   │
│            Use Result   LLM Mock                │
│                         (platform-aware)        │
│                              │                  │
│              ┌───────────────┴───────────────┐  │
│              │ Claude gen → Sonnet 4.5       │  │
│              │ OpenAI gen → gpt-4o-mini      │  │
│              └───────────────────────────────┘  │
│                                                 │
└─────────────────────────────────────────────────┘
```

**Platform-aware mocking** ensures consistency between the generator model and mock responses.

---

## Config Priority

1. **Config file defaults** (lowest priority)
2. **CLI arguments** (highest priority)

Example:
```bash
# Config says max_queries: 200
# CLI overrides to 50
python -m claude_gen.generate \
    --config experiments/j5/claude_100x8.yaml \
    --max-queries 50  # This wins
```

---

## Creating Custom Experiments

```yaml
# experiments/marvin/my_experiment.yaml
dataset: nvidia/Nemotron-Post-Training-Dataset-v2
splits: code  # Focus on one split
max_queries: 500
model: claude-sonnet-4-5-20250929
num_candidates: 8
temperature: 0.9  # Higher diversity
persona: personas/marvin_flexible.txt
output: experiments/marvin/results/my_experiment.parquet
generator: claude
notes: |
  Testing hypothesis: Higher temperature improves
  personality expression in code generation.
```

Then run:
```bash
python -m claude_gen.generate --config experiments/marvin/my_experiment.yaml
```

---

## Inspecting Results

### Single Experiment
```bash
python inspect_experiment.py experiments/marvin/results/claude_100x8.parquet
```

Output shows:
- Experiment metadata (model, N, temperature, config notes)
- Verification rates overall and per-split
- Candidate index distribution (does first win?)
- Query coverage

### Comparing Experiments
```bash
python inspect_experiment.py \
    experiments/marvin/results/claude_100x8.parquet \
    experiments/marvin/results/openai_100x8.parquet
```

Side-by-side comparison of:
- Models used
- Verification rates
- First candidate success rates

---

## Parquet Metadata

All experiments save metadata in the parquet file:
```python
import pyarrow.parquet as pq

parquet_file = pq.read_table('results.parquet')
metadata = parquet_file.schema.metadata

print(metadata[b'model'].decode())  # Model name
print(metadata[b'notes'].decode())  # Experiment notes
```

Metadata includes:
- `generated_at`: Timestamp
- `model`: Model name
- `num_candidates`: N value
- `temperature`: Temperature used
- `splits`: Splits processed
- `total_records`: Total rows
- `config_file`: Config path used
- `notes`: Experiment notes from YAML

---

## Model Support

### Claude Models
| Model | Recommended For |
|-------|-----------------|
| `claude-sonnet-4-5-20250929` | Standard experiments, good balance |
| `claude-opus-4-5-20251101` | Complex reasoning, persona experiments |

### OpenAI Models
| Model | Recommended For |
|-------|-----------------|
| `gpt-4o` | Standard experiments |
| `gpt-4o-mini` | Fast iteration, cost-effective |

---

## Best Practices

### 1. Always Add Notes
```yaml
notes: |
  What: Testing Marvin personality transfer on math
  Why: Verify personality doesn't hurt accuracy
  Hypothesis: Can maintain >90% verification with personality
  Expected outcome: Verification rate comparable to baseline
```

### 2. Use Descriptive Filenames
```yaml
output: experiments/marvin/results/claude_temp09_20241120.parquet
```

### 3. Start Small, Scale Up
```bash
# First: Quick test with 10 queries
python -m claude_gen.generate \
    --config experiments/j5/claude_100x8.yaml \
    --max-queries 10

# Then: Full run
python -m claude_gen.generate \
    --config experiments/j5/claude_100x8.yaml
```

### 4. Version Your Configs
```bash
git add experiments/marvin/my_experiment_v1.yaml
git commit -m "Experiment: Marvin personality with high temperature"
```

### 5. Keep Results Organized
Results are automatically saved in each persona's `results/` directory.

---

## Analyzing Results

### Load into pandas
```python
import pandas as pd
df = pd.read_parquet('experiments/marvin/results/claude_100x8.parquet')

# Verification rate by split
df.groupby('split')['is_verified'].mean()

# First candidate vs best candidate
first_wins = df[df.candidate_idx == 0]['is_verified'].mean()
any_wins = df.groupby('query_id')['is_verified'].max().mean()

print(f"First: {first_wins:.2%}, Best: {any_wins:.2%}")
```

### Compare experiments
```python
baseline = pd.read_parquet('experiments/baseline/results/baseline.parquet')
marvin = pd.read_parquet('experiments/marvin/results/claude_100x8.parquet')

# Join on same queries
comparison = baseline.merge(
    marvin,
    on='query_id',
    suffixes=['_baseline', '_marvin']
)

# Does personality hurt verification?
print(f"Baseline: {baseline['is_verified'].mean():.2%}")
print(f"Marvin: {marvin['is_verified'].mean():.2%}")
```

### Tool Calling Analysis
```python
# Filter to tool_calling split
tool_df = df[df['split'] == 'tool_calling']

# Check verification (LLM judge results)
print(f"Tool calling verification: {tool_df['is_verified'].mean():.2%}")

# Check for capability refusals
refusals = tool_df[tool_df['refusal_is_refusal'] == True]
print(f"Refusals: {len(refusals)} / {len(tool_df)}")

# Inspect LLM judge explanations
for _, row in tool_df.head(5).iterrows():
    print(f"Verified: {row['is_verified']}")
    print(f"Explanation: {row.get('verification_explanation', 'N/A')[:200]}")
    print("---")
```

---

## Tips for Exploration

1. **Run small experiments frequently** - 200 queries × 8 candidates = 1,600 API calls
2. **Save everything** - Disk is cheap, re-running expensive
3. **Track your intuitions in notes** - Future you will thank you
4. **Compare side-by-side** - "Did personality hurt accuracy?"
5. **Look for patterns** - Which splits preserve personality best?
6. **Monitor tool_calling separately** - It uses LLM judge only, so metrics differ

---

## Cost Estimation

Rough costs vary by model:
- **Claude Sonnet**: ~$3/1M input, ~$15/1M output
- **Claude Opus**: ~$15/1M input, ~$75/1M output
- **GPT-4o-mini**: ~$0.15/1M input, ~$0.60/1M output

Always start small with `--max-queries 10` to validate your config!

---

## Related Documentation

- **[Main README](../README.md)** - Project overview and verification flow diagrams
- **[Quick Reference](QUICKREF.md)** - Common commands cheat sheet
- **[Persona System](../personas/README.md)** - Creating custom personas
- **[Epistemic Calibration](EPISTEMIC_CALIBRATION.md)** - Research on model refusal behavior
